Conversational Agent is implemented using LangChain. LangChain is a framework for building applications powered by large language models (LLMs).

For the knowledge base, Pinecone vector database is used. Langchain supports Pinecone integration out of the box. Using vector database allows to embed, store and retrieve additional knowledge efficiently.


Google search is implemented using SerpAPI, which is also integrated into Langchain.


The hardest part is using both knowledge base and Google search in Conversational Agent.
We use ReAct approach in which the Agent is provided with a set of tools that can be used to accomplish user request. The problem is that when user asks non-general queries, the agent do not know which tool to use: knowledge base(s) or Google search.

There are two possible solutions:
1. The agent selects a tool based on a query and tool description. To help the agent 
to select a correct tool (in this cases knowledge base or Google search), each knowledge base tool needs to be described as best as possible. This approch doesnt need manual annotations, as the description(s) can be generated by LLM. 

2. The agent have no problem with distinguishing between general queries and the ones that require Google search or knowledge base. In this case, the seach tool can be implemented as a separate custom chain that first checks if the query can be adressed by retrived information from knowledge base and if not, the query is passed to Google search.

For this project, the first approach is used, as it requires less modifications of avaliable Langchain components.


Connection to a support agent can be triggered by user in two ways:
1. By writing "\h"
2. By writing a query that contains information about user's intent to connect to a support agent. For example, "I need to talk to a human" or "I need to talk to a support agent".
3. When an exception is raised, the agent asks user if he wants to connect to a support agent.